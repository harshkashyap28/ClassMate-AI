{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "1. Using Beacon technology Akash Hake Department of Electronics Engineering Sardar Patel Institute of Technology Mumbai, India rajendra sutar@spit.ac.in Amolina Samanta Department of Electronics Engineering Sardar Patel Institute of Technology Mumbai, India prashant Kasambe Department of Electronics Engineering Sardar Patel Institute of Technology Mumbai, India rajendra sutar@spit.ac.in A vast database would be generated that would include user information pertaining to the attendance.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (This should only happen once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    # You can further customize these patterns based on the content of your PDF.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections (adjust to the specific structure of your document)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file (Replace with the correct path)\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Automatic_Attendance_Marker_Using_Beacon_technology.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata and headers\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=400, min_length=100, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary in point-wise format\n",
    "print(\"\\nSummary:\")\n",
    "# Improved handling for sentence splitting\n",
    "summary_points = decoded_summary.split('. ')\n",
    "for i, point in enumerate(summary_points, start=1):\n",
    "    print(f\"{i}. {point.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: ''/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0_/q7hlfhcn723fzh10jym7md6c0000gn/T/ipykernel_40821/2915814304.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Path to your PDF file (Replace with the correct path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mpdf_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Extract text from the provided PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mpdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Preprocess the extracted text to remove unnecessary metadata and headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0_/q7hlfhcn723fzh10jym7md6c0000gn/T/ipykernel_40821/2915814304.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Open the PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract text from each page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   3072\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count_pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3076\u001b[0;31m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: ''/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf''"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (This should only happen once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\", \"Author\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections (adjust to the specific structure of your document)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # Skip lines containing unwanted headers or metadata like email addresses or names\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "    \n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file (Replace with the correct path)\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Automatic_Attendance_Marker_Using_Beacon_technology.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata and headers\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,     # Increase max_length for longer summary\n",
    "    min_length=150,     # Ensure a minimum summary length\n",
    "    num_beams=6,        # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7     # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "1. based on BLE beacons, a web service and a mobile application\n",
      "2. This paper proposes the design of a smart attendance man- agement system based on BLE beacons\n",
      "3. The design is based on three interacting components—BLE beacons, a web service and a mobile application\n",
      "4. This paper proposes the design of an automatic attendance management system that addresses the problem of student attendance tracking\n",
      "5. The design is based on three interacting components—BLE beacons, a web service\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (This should only happen once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata (like titles or references)\n",
    "def preprocess_text(text):\n",
    "    # You can remove certain known patterns, e.g., author names, header/footer text\n",
    "    # Example: Remove metadata at the start (adjust based on your document's structure)\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Assuming that metadata like author names or title are in the first few lines\n",
    "    lines = lines[5:]  # Skip the first 5 lines (you can adjust this based on your PDF)\n",
    "    \n",
    "    return \" \".join(lines)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=400, min_length=100, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary in point-wise format\n",
    "print(\"\\nSummary:\")\n",
    "# Improved handling for sentence splitting\n",
    "summary_points = decoded_summary.split('. ')\n",
    "for i, point in enumerate(summary_points, start=1):\n",
    "    print(f\"{i}. {point.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      ". This paper proposes the design of a smart attendance management system with Bluetooth low energy beacons. The design incorporates three main components that interact to deliver a seamless user experience. The design is based on three interacting components—BLE beacons, a web service and a mobile application. The design is based on three interacting components—BLE beacons, a web service and a mobile application. This paper proposes the design of an automatic attendance management system that addresses the problem of student  e- posal of the design of a. The....... The. The. The. The.... The. The. The. The. The.. The. The. The.. The. The. The. The. The. The.. The. The. The. The.. The. The.. The. The.. The. The.. The. The.. The.. The.. The.. The.. The. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The.. The... The.. The.. The...... The.. The... The...... The... The... The... The.........\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (This should only happen once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata (like titles or references)\n",
    "def preprocess_text(text):\n",
    "    # You can remove certain known patterns, e.g., author names, header/footer text\n",
    "    # Example: Remove metadata at the start (adjust based on your document's structure)\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections (adjust to the specific structure of your document)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file (Replace with the correct path)\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata and headers\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,    # Increased max length for more detailed summaries\n",
    "    min_length=150,    # Minimum length for the summary\n",
    "    num_beams=6,       # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7    # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "# Improved handling for sentence splitting\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ed41bf6734ad696a7a56fe52401f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   3%|3         | 52.4M/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737570030&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzU3MDAzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=aQYj33sMetKTfP%7E5Jfz6BBXwlQAEiVpIxmyTvgqi1vN%7E1xOkaQWoT2YJe0Miw2bcx-uTT2-AuDx%7ETJuHfNxwluq2vl-NF-t-zPmamAK8OksYWOUhqjL5ya%7ETwO8XLnWhUCOrRLYfwD1cuW-YUUX-UpjS3jcZrlibJqr1MccRNpPo9gw9IMgYSRn8PSGh5Q597rK8MDwh9UDbHX2n17yloA4NpB76Puh082XkGVOpRe7VwPRwDnLovSh7-AZHjGw7laC9EzAa9vr8m7YZQ6MrSbmC7WcA7dbWAOl9sYpnF%7ESznuWHekt6bQgIwAcDqi0PmtQqtb5AK7VZrw5YWvQ3iQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb84713c84e43a5a9185a56b286becd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   4%|3         | 62.9M/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737570030&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzU3MDAzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=aQYj33sMetKTfP%7E5Jfz6BBXwlQAEiVpIxmyTvgqi1vN%7E1xOkaQWoT2YJe0Miw2bcx-uTT2-AuDx%7ETJuHfNxwluq2vl-NF-t-zPmamAK8OksYWOUhqjL5ya%7ETwO8XLnWhUCOrRLYfwD1cuW-YUUX-UpjS3jcZrlibJqr1MccRNpPo9gw9IMgYSRn8PSGh5Q597rK8MDwh9UDbHX2n17yloA4NpB76Puh082XkGVOpRe7VwPRwDnLovSh7-AZHjGw7laC9EzAa9vr8m7YZQ6MrSbmC7WcA7dbWAOl9sYpnF%7ESznuWHekt6bQgIwAcDqi0PmtQqtb5AK7VZrw5YWvQ3iQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c49653520d4cc1bd4ea2cef82bd6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  12%|#2        | 199M/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737570030&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzU3MDAzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=aQYj33sMetKTfP%7E5Jfz6BBXwlQAEiVpIxmyTvgqi1vN%7E1xOkaQWoT2YJe0Miw2bcx-uTT2-AuDx%7ETJuHfNxwluq2vl-NF-t-zPmamAK8OksYWOUhqjL5ya%7ETwO8XLnWhUCOrRLYfwD1cuW-YUUX-UpjS3jcZrlibJqr1MccRNpPo9gw9IMgYSRn8PSGh5Q597rK8MDwh9UDbHX2n17yloA4NpB76Puh082XkGVOpRe7VwPRwDnLovSh7-AZHjGw7laC9EzAa9vr8m7YZQ6MrSbmC7WcA7dbWAOl9sYpnF%7ESznuWHekt6bQgIwAcDqi0PmtQqtb5AK7VZrw5YWvQ3iQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8937bd7777f4553a181a0821078e5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  25%|##5       | 409M/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cd840540cd464ba8848eb57401753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Design and development of a smart attendance management system with Bluetooth low energy beacons. Tracking and managing student attendance during lectures and exams is an especially important instance of that task. This paper proposes the design of an automatic system tailored for the tracking and management of student attendance data. The design incorporates three main components that interact to deliver a seamless user experience. The key element is the utilisation of simple beacons, which make the system cost-effective and easy to use. Combining them with a mobile platform and a web service enables versatile attendance tracking. An important aspect of the system is its ability to deal with students leaving before the end of the lecture or attempting an inappropriately late attendance. Speciﬁcally, keeping track of attendance in different time points during lectures enables insight into student presence.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (BART model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Design_and_development_of_a_smart_attendance_management_system_with_Bluetooth_low_energy_beacons.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,    # Increased max length for more detailed summaries\n",
    "    min_length=150,    # Minimum length for the summary\n",
    "    num_beams=6,       # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7    # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Smart Attendance Management using Bluetooth Low Energy and Android. In most schools and universities in India, a minimum attendance requirement is present and the teacher manually records the attendance of the students present in the class. The solution presented in the paper is a smart and fast attendance monitoring system through Bluetooth low energy sensors. These sensors can be attached to each identity card. They contain a unique string which can be associated with the id card they are attached to. This is coupled with an android application on the teacher’s mobile phone which is used to collect the data. The app provides different modes of visualizing the data such as a list mode and a pie chart mode for easy analyses. To avoid any kind of error or false attendance, sensors are used to count the number of people in the classroom.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (BART model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Smart_attendance_management_using_Bluetooth_Low_Energy_and_Android.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,    # Increased max length for more detailed summaries\n",
    "    min_length=150,    # Minimum length for the summary\n",
    "    num_beams=6,       # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7    # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Smart Attendance Management using Bluetooth Low Energy and Android. In most schools and universities in India, a minimum attendance requirement is present and the teacher manually records the attendance of the students present in the class. The solution presented in the paper is a smart and fast attendance monitoring system through Bluetooth low energy sensors. These sensors can be attached to each identity card. They contain a unique string which can be associated with the id card they are attached to. This is coupled with an android application on the teacher’s mobile phone which is used to collect the data. The app provides different modes of visualizing the data such as a list mode and a pie chart mode for easy analyses. To avoid any kind of error or false attendance, sensors are used to count the number of people in the classroom.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (BART model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file_path = \"/Users/harshkumar/mac/Final Year project/research paper/Smart_attendance_management_using_Bluetooth_Low_Energy_and_Android.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,    # Increased max length for more detailed summaries\n",
    "    min_length=150,    # Minimum length for the summary\n",
    "    num_beams=6,       # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7    # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "ClassMate AI is an easy-to-use platform that helps manage classroom tasks using AI. It automates attendance with selfies, summarizes notes, generates quizzes from the material,  and sends reminders for assignments. Built with React and Python, ClassMateAI makes learning  and classroom management simpler and  more efficient. It is available for iOS, Android, Windows, Mac, Linux, Mac OS X, Windows Phone, iPad, and Android Wear. For more information, visit classmate AI.com or go to: http://www.classmate.com/classmate-ai.html. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for text summarization (BART model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Pre-process text by removing unnecessary metadata and section headers\n",
    "def preprocess_text(text):\n",
    "    # Remove section headers like \"Abstract\", \"Introduction\", etc.\n",
    "    unwanted_headers = [\"Abstract\", \"Introduction\", \"Objective\", \"Conclusion\", \"References\"]\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove unwanted sections\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(header in line for header in unwanted_headers):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join the cleaned lines back into a single text block\n",
    "    return \" \".join(filtered_lines)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file_path = \"/Users/harshkumar/Downloads/Project Synopsis Formats PES1PG23ca051.pdf\"\n",
    "\n",
    "# Extract text from the provided PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Preprocess the extracted text to remove unnecessary metadata\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Tokenize the processed text (if it's too long, truncate to the max length)\n",
    "inputs = tokenizer(processed_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary using the model (adjusted max_length and num_beams for better quality)\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=600,    # Increased max length for more detailed summaries\n",
    "    min_length=150,    # Minimum length for the summary\n",
    "    num_beams=6,       # Higher num_beams for better quality\n",
    "    early_stopping=True,\n",
    "    temperature=0.7    # Control randomness in the output (lower temperature for more deterministic output)\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary as a continuous text block\n",
    "print(\"\\nSummary:\")\n",
    "print(decoded_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream.functions import MapFunction\n",
    "import json\n",
    "\n",
    "class GenerateAlert(MapFunction):\n",
    "    def map(self, value: str) -> list:\n",
    "        # Parse the JSON input\n",
    "        data = json.loads(value)\n",
    "        alerts = []\n",
    "        \n",
    "        # Process batsmen data for boundary, 50, and 100\n",
    "        for batsman in data.get('batsmen', []):\n",
    "            if batsman.get('fours', 0) > 0:\n",
    "                alerts.append(f\"{batsman['name']} scored a boundary.\")\n",
    "            if batsman.get('runs', 0) >= 50:\n",
    "                alerts.append(f\"{batsman['name']} scored a 50.\")\n",
    "            if batsman.get('runs', 0) >= 100:\n",
    "                alerts.append(f\"{batsman['name']} scored a 100.\")\n",
    "\n",
    "        # Process bowler data for wickets\n",
    "        for bowler in data.get('bowlers', []):\n",
    "            if bowler.get('wickets', 0) > 0:\n",
    "                alerts.append(f\"{bowler['name']} took a wicket.\")\n",
    "\n",
    "        return alerts\n",
    "# Create StreamExecutionEnvironment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Kafka Consumer to read data from Kafka topic\n",
    "kafka_source = FlinkKafkaConsumer(\n",
    "    topics=\"match_data_topic\",\n",
    "    deserialization_schema=lambda x: x.decode('utf-8'),\n",
    "    properties={\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"matchgroup\"}\n",
    ")\n",
    "# Create a DataStream from Kafka Source\n",
    "data_stream = env.add_source(kafka_source)\n",
    "\n",
    "# Apply the transformation to generate alerts\n",
    "alert_stream = data_stream.map(GenerateAlert(), output_type=Types.LIST(Types.STRING()))\n",
    "\n",
    "# Kafka Producer to send the alerts to Kafka topic\n",
    "kafka_sink = FlinkKafkaProducer(\n",
    "    topic=\"alert_topic\",\n",
    "    serialization_schema=lambda x: json.dumps(x).encode('utf-8'),\n",
    "    producer_config={\"bootstrap.servers\": \"localhost:9092\"}\n",
    ")\n",
    "# Sink the alerts to Kafka\n",
    "alert_stream.add_sink(kafka_sink)\n",
    "\n",
    "# Execute the job\n",
    "env.execute(\"Match Data Alerts\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
